---
title: "Text analysis 3"
author: "Junyan Yao"
date: "10/16/2017"
output: html_document
---

Research question: Whether chat engagement is associated with test outcomes(see how students' collaborative performance can be associated with students' performance in these math problems).

Data: Chat data and test outcome data

```{r}

#load the chat data
data<- read.csv("~/Documents/NYU/Fall 2017/Text Analysis Project/cpsv_text_project/chat_time_series.csv")
data<- data[,c(2,5,8)] #extract needed column
head(data)

#subset the data
chatdata<- data[which(data$type=="chat"),] #this is what we want to look at for now
problemdata<- data[which(data$type=="problem"),]
head(chatdata)

#load the outcome data
outcomedata<-read.csv("~/Documents/NYU/Fall 2017/Text Analysis Project/cpsv_text_project/group_outcomes.csv") 

head(outcomedata)
subset1<- outcomedata[outcomedata$group_id>0,] #this will get rid of all negative group_id

summary(subset1$delta)

performance<-ifelse(subset1$delta>0.4058,"high",ifelse(subset1$delta< -0.481,"low","in-between"))
temp22<-cbind(subset1,performance)
#try to get rid of the missing rows(some group id are missing in the outcome data)
merged_data<- merge(x=chatdata,y=temp22,by="group_id")

#now back to these two dataset, treat the merged_data as the chat data.
#chatdata2<- merged_data[,1:3]
#head(chatdata2)
```



```{r}
#split to two groups- High performance group and low performance group;
high_group<- merged_data[which(merged_data$performance=="high"),]
low_group<- merged_data[which(merged_data$performance=="low"),]

#get the most common non-punctuation, non-stop word terms in the chat
Y<- term_stats(merged_data$content, drop=stopwords_en, drop_punct=TRUE) #the support is the number of texts containing the term.
# by using drop= stopwords_en, we can exclude these "functional" words
head(Y, 10)

Y_high<- term_stats(high_group$content,drop=stopwords_en, drop_punct=TRUE)
Y_low<- term_stats(low_group$content,drop=stopwords_en, drop_punct=TRUE)

S<- subset(Y, Y$support>5)
S_high<-subset(Y_high,Y_high$support>5)
S_low<-subset(Y_low,Y_low$support>5)

#probably not drop the "functional" words
YY<- term_stats(merged_data$content)
head(YY,10)
YY_high<- term_stats(high_group$content)
head(YY_high,10)
YY_low<-term_stats(low_group$content)
head(YY_low, 10)


#higher-order n-grams
term_stats(merged_data$content,ngrams = 3)
term_stats(merged_data$content,ngrams = 4)
term_stats(merged_data$content,ngrams = 5)

term_stats(high_group$content,ngrams = 4)
term_stats(low_group$content,ngrams = 4)



```


```{r}
#Emotion-Lexicon
affect<- subset(affect_wordnet,emotion != "Neutral")
affect$emotion<- droplevels(affect$emotion) #drop the unused neutral level
affect$category<- droplevels(affect$category) #drop unused categories

term_stats(merged_data$content, subset = term %in% affect$term)
term_stats(high_group$content, subset = term %in% affect$term)
term_stats(low_group$content, subset = term %in% affect$term)


text_sample(high_group$content,"hard")
text_sample(low_group$content,"hard")
#term emotion matrix
#segment the text into smaller chunks and then compute the emotion occurence rates in each chunk, broken down by category ("positive","negative","ambiguous")

term_score<- with(affect, unclass(table(term,emotion))) 
head(term_score) #while not very informative
```


don't run this


```{r}
termS<- YY$term

for (i in 1: length(termS)){
 yy_high_count<- YY_high[match(YY_high$term,termS),]
  yy_low_count<- YY_low[match(YY_low$term,termS),]
}

yy_high_count<- yy_high_count[,-3] #drop the support column
yy_low_count<- yy_low_count[,-3] #drop the support column

dat<- merge(yy_high_count,yy_low_count,by="term",all=TRUE)

```


try another way here:




```{r}
pronoun<- c("high","low")
#bigram_counts_high<- term_stats(high_group$content,ngrams = 3,types = TRUE, subset= type1 %in% pronoun)
#not very meaningful



#rearrange the data into tabular form, with one row for each term and two columns

terms<- with(bigram_counts_high,tapply(count, list(type2, type1), identity)) #this is not informative
print(terms)
#gender-specific usage rates

term<- "think"
i<- match(term, rownames(terms))
tab<- cbind(terms[i,], colSums(terms[-i,]))
colnames(tab)<- c(term, paste0("\u00ac",term))
print(tab)

```








Tokenize data
```{r}
#Not run:
#load packages
library(corpus)
library(Matrix)
library(tm) #data import, corpus handling, preprocessing, metadata management, and creation of term-document matrix


#Tokenize chat data (compute the occurrence counts for each term, returning the result as a sparse matrix(text-by-terms), term_counts returns the same information, but in a data frame).

#tabulating a matrix of text-term occurrence counts
M<- term_matrix(chatdata2$content, group = chatdata2$group_id) #this is sparse matrix 

C<- term_counts(chatdata2$content, group = chatdata2$group_id) #this is data frame


#df<- as.data.frame(cbind(subset1$delta, M))
#this did not work: sparse matrix cannot be converted to data frame directly, seems

temp<-as.data.frame(as.matrix(M)) #this converted the sparse matrix to regular matrix and then convert to data frame
dim(temp)

subset3<- as.data.frame(cbind(subset1$delta, temp))
#the first column is delta

#find the top quantile and low quantile of delta

summary(subset3$`subset1$delta`)
```

Now we define the top performance groups are these with delta greater than 0.4058; the poor performance groups are these with delta lower than -0.481;


Split the data to top performance groups and poor performance groups:
```{r}
top_perf<-subset3[which(subset3$`subset1$delta`>0.4058),]
poor_perf<-subset3[which(subset3$`subset1$delta`< -0.481),]

dim(top_perf)
dim(poor_perf)
```

